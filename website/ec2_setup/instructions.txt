
db.createCollection('listing_sups')
db.createCollection('listing_unsups')
db.createCollection('listing_u_sups')
db.createCollection('listing_u_unsups')
db.createCollection('loc_sups')
db.createCollection('loc_unsups')
db.createCollection('prod_sups')
db.createCollection('prod_unsups')


db.createCollection('listing_loc_sups')
db.createCollection('listing_loc_unsups')
db.createCollection('listing_prod_sups')
db.createCollection('listing_prod_unsups')

python load.py 'both' True True '2015-03-19' '2015-03-19'




Hey team, 

Need some help. Also thought to give a quick update. Let me first explain how the latest database script for S3—> mongodb works first:

SETUP ON YOUR MONGO DB
- First, make sure you pull all the code from Github
- Go to you Mongo console and type the following:

use w205project
db.dropDatabase()    <— erase your original database so we can start fresh
use w205project
db.createCollection('listing_sups’)          
db.createCollection('listing_unsups’)            <— notice now we have two collection one for supervised (sup), one for unsupervised (unsup)
db.createCollection('listing_u_sups’)            <— this collection means that we have a unique listing (i.e. the de-dup list)
db.createCollection('listing_u_unsups')
db.createCollection('loc_sups')
db.createCollection('loc_unsups’)                 <— now we need two different collections to store the location data depending on supervised or unsupervised
db.createCollection('prod_sups’) 
db.createCollection('prod_unsups’)              <— similar for product.. two different collections now


INSTRUCTIONS ON HOW TO RUN THIS NEW PYTHON SCRIPT
- Example 1: load data for both supervised and unsupervised databases (erase previous data) for all time periods
  -- Run this on command line…     python load.py both T T ''  ''        <— there are 4 single quotes (not two double quotes)

- Example 2: load data for both supervised database (erase previous data) for period from 2015-03-19 to 2015-03-23
  -- Run this on command line…     python load.py supervised T T 2015-03-19 2015-03-23

- Example 3: load data for both supervised database (WITHOUT erasing all previous data) for day of 2015-03-30
  -- Run this on command line…     python load.py supervised T F 2015-03-30 2015-03-30

I think you would run “Example 1” when you load it the first time. 
Then, you would run “Example 3” on each new subsequent day.


HELP:
- Can someone help make this an automated script / cron job that works on your local machine? 
- Assuming you have already run “Example 1” on 2015-04-18 and grabbing all the data, the automated script should behave as follow: 
DAY 1:      python load.py supervised T F 2015-04-19 2015-04-19
DAY 2:      python load.py supervised T F 2015-04-20 2015-04-20
DAY 3:      python load.py supervised T F 2015-04-21 2015-04-21
etc…

Thanks a million! I have been spending way too much time figuring out this script. Need to focus on making all the website changes now. 

SUMMARY OF WHAT’S CHANGED ON THE PYTHON SCIRPT (load.py):
- made it such that it can take on external argument input
- made it allow incremental daily input vs needing to erase/enter all data to database every time
- made it load a lot faster (probably 10x faster). It has been an issue when we load close to 1 million listing info. I can explain more later what I have done to make it faster as think it would be good info for you all. 
- fixed issue with finding unique listing with close to 1 million listings info… mongodb needs to be modified when you want to perform aggregation on a very large dataset. I can explain more
- re-setup the two new set of collections structure for supervised and unsupervised properly
- changed the use of scrape date instead of create date for the C3 plots
- configure the dates properly so that we no longer has time-zone issues. As far as our project is concerned, we are always working in UTC

Thanks, let me focus on the website stuff now. 

Arthur